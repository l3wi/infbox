version: "3.9"

services:
  vllm:
    command: >
      /models/Qwen_Qwen2.5-Coder-32B-Instruct-AWQ
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size ${GPU_COUNT:-1}
      --dtype ${DTYPE:-auto}
      --max-model-len ${MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${GPU_UTIL:-0.85}
      --enable-prefix-caching
      --enforce-eager
    environment:
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True