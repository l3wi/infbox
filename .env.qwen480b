# Qwen3-Coder-480B-A35B-Instruct-FP8 Configuration
# Optimized for multi-GPU inference with FP8 quantization

# Model configuration
MODEL_NAME=Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
MODEL_PATH=/models/Qwen_Qwen3-Coder-480B-A35B-Instruct-FP8
PREC=bfloat16
GPU_COUNT=4
GPU_UTIL=0.97
CUDA_DEVICES=0,1,2,3

# Memory configuration
CPU_GB=0
DISK_GB=100

# vLLM configuration
MAX_MODEL_LEN=238000
DTYPE=bfloat16
KV_CACHE_DTYPE=fp8
QUANTIZATION=fp8
ATTENTION_BACKEND=FLASH_ATTN
VLLM_USE_V1=1
TORCH_CUDA_ARCH_LIST=9.0

# Service ports
VLLM_PORT=8000

# Workspace
WORKSPACE_DIR=/workspace
WATCH_INTERVAL=1

# Model storage
MODELS_PATH=/models

# Logging
LOG_LEVEL=INFO

# Optional: Hugging Face token for gated models
# HF_TOKEN=your_token_here