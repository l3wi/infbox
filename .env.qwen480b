# Qwen3-Coder-480B-A35B-Instruct-FP8 Configuration
# Optimized for multi-GPU inference with FP8 quantization

# Model configuration
MODEL_NAME=Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
MODEL_PATH=/models/Qwen_Qwen3-Coder-480B-A35B-Instruct-FP8
PREC=fp8
GPU_COUNT=4
GPU_UTIL=0.90
CUDA_DEVICES=0,1,2,3

# Memory configuration
CPU_GB=40
DISK_GB=100

# vLLM configuration
MAX_MODEL_LEN=32768
DTYPE=float16
KV_CACHE_DTYPE=fp8
QUANTIZATION=fp8
ATTENTION_BACKEND=FLASH_ATTN

# Service ports
VLLM_PORT=8000

# Workspace
WORKSPACE_DIR=/workspace
WATCH_INTERVAL=1

# Model storage
MODELS_PATH=/models

# Logging
LOG_LEVEL=INFO

# Optional: Hugging Face token for gated models
# HF_TOKEN=your_token_here