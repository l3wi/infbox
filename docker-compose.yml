services:
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      ${MODEL_PATH:-/models/Qwen_Qwen3-Coder-480B-A35B-Instruct-FP8}
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size ${GPU_COUNT:-4}
      --dtype ${DTYPE:-float16}
      --max-model-len ${MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${GPU_UTIL:-0.90}
      --quantization ${QUANTIZATION:-fp8}
      --kv-cache-dtype ${KV_CACHE_DTYPE:-fp8}
      --cpu-offload-gb ${CPU_GB:-0}
      --enable-prefix-caching
      --trust-remote-code
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${MODELS_PATH:-./models}:/models:ro
      - ./cache:/cache
      - ${WORKSPACE_DIR:-/root}:${WORKSPACE_DIR:-/root}:ro
    ports:
      - "${VLLM_PORT:-8000}:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0,1,2,3}
      - HF_HOME=/models
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - VLLM_ATTENTION_BACKEND=${ATTENTION_BACKEND:-FLASH_ATTN}
      - VLLM_USE_V1=${VLLM_USE_V1:-1}
      - TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST:-9.0}
    networks:
      - llm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  watcher:
    build:
      context: .
      dockerfile: docker/Dockerfile.watcher
    volumes:
      - ${WORKSPACE_DIR:-/root}:${WORKSPACE_DIR:-/root}:ro
      - ./scripts:/scripts:ro
      - ./config/watcher-ignore:/etc/watcher-ignore:ro
    environment:
      - WATCH_DIR=${WORKSPACE_DIR:-/root}
      - IGNORE_FILE=/etc/watcher-ignore
      - VLLM_ENDPOINT=http://vllm:8000
      - WATCH_INTERVAL=${WATCH_INTERVAL:-1}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - EXTRA_IGNORE_DIRS=infbox,models
    networks:
      - llm-network
    restart: unless-stopped
    depends_on:
      - vllm

  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
      - "5555:5555"
    volumes:
      - ./config/Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - llm-network
    restart: unless-stopped

networks:
  llm-network:
    driver: bridge

volumes:
  caddy_data:
  caddy_config: